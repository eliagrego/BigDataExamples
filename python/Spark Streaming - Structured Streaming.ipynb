{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\n\n# This works with the stream generated by the following command on unix systems: `nc -lk 9999'\n# Create DataFrame representing the stream of input lines from connection to localhost:9999\nlines = spark \\\n    .readStream \\\n    .format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# The function explode creates a new row for each element of the resulting list from the split operation\n\n# Split the lines into words\nwords = lines.select(\n   explode(\n       split(lines.value, \" \")\n   ).alias(\"word\")\n)\n\n# Generate running word count\nwordCounts = words.groupBy(\"word\").count()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Group the data by window and word and compute the count of each group\nwindowedCounts = words \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n        words.word) \\\n    .count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":[" # Start running the query that prints the running counts to the console\nquery = wordCounts \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\n\nquery.awaitTermination()"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"Spark Streaming - Structured Streaming","notebookId":1230649527278660},"nbformat":4,"nbformat_minor":0}
